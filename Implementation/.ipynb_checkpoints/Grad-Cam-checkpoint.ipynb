{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for InES Seminar Explaibiibility of AI in image classification context\n",
    "# 19.02.2022\n",
    "# GitHub link: https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py\n",
    "#              https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/patrickknab/opt/anaconda3/envs/INES/lib/python3.8/site-packages/keras/layers/normalization.py:524: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import (\n",
    "    VGG19, preprocess_input, decode_predictions)\n",
    "\n",
    "from keras.applications.vgg16 import (\n",
    "    VGG16, preprocess_input, decode_predictions)\n",
    "\n",
    "import lime\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from keras.applications import inception_v3 as inc_net\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.framework import ops\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import sys\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "inet_model = inc_net.InceptionV3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_category_loss(x, category_index, nb_classes):\n",
    "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
    "\n",
    "def target_category_loss_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "def load_image(path):\n",
    "    #img_path = sys.argv[1]\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img).astype(np.uint8)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) * \\\n",
    "                tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='block5_conv3'):\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "\n",
    "def modify_backprop(model, name, mods):\n",
    "    g =  tf.compat.v1.get_default_graph()\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "\n",
    "        # replace relu activation\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == keras.activations.relu:\n",
    "                layer.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        if mods == 19:\n",
    "            new_model = VGG19(weights='imagenet')\n",
    "        elif mods == 16:\n",
    "            new_model = VGG16(weights='imagenet')\n",
    "\n",
    "\n",
    "    return new_model\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if keras.backend.image_data_format() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def _compute_gradients(tensor, var_list):\n",
    "    grads = tf.gradients(tensor, var_list)\n",
    "    return [grad if grad is not None else tf.zeros_like(var) for var, grad in zip(var_list, grads)]\n",
    "\n",
    "def grad_cam(input_model, image, category_index, layer_name):\n",
    "    nb_classes = 1000\n",
    "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
    "    x = Lambda(target_layer, output_shape = target_category_loss_output_shape)(input_model.output)\n",
    "    model = Model(inputs=input_model.input, outputs=x)\n",
    "    #model.summary()\n",
    "    loss = K.sum(model.output)\n",
    "    conv_output =  [l for l in model.layers if l.name is layer_name][0].output\n",
    "    grads = normalize(_compute_gradients(loss, [conv_output])[0])\n",
    "    gradient_function = K.function([model.input], [conv_output, grads])\n",
    "\n",
    "    output, grads_val = gradient_function([image])\n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "\n",
    "    weights = np.mean(grads_val, axis = (0, 1))\n",
    "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * output[:, :, i]\n",
    "\n",
    "    cam = cv2.resize(cam, (224, 224))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    heatmap = cam / np.max(cam)\n",
    "\n",
    "    #Return to BGR [0..255] from the preprocessed image\n",
    "    image = image[0, :]\n",
    "    image -= np.min(image)\n",
    "    image = np.minimum(image, 255)\n",
    "\n",
    "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "    cam = np.float32(cam) + np.float32(image)\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    return np.uint8(cam), heatmap\n",
    "\n",
    "def limePics(path):\n",
    "\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img).astype(np.uint8)\n",
    "    x1 = np.expand_dims(x, axis=0)\n",
    "    x1 = preprocess_input(x1)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    model = VGG16(weights='imagenet')\n",
    "\n",
    "    \n",
    "    preds = model.predict(x1)\n",
    "    for i in decode_predictions(preds)[0]:\n",
    "        print(i)\n",
    "    explanation = explainer.explain_instance(x.astype('double'), \n",
    "                                             model.predict, \n",
    "                                             top_labels=5, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000)\n",
    "    return explanation\n",
    "\n",
    "\n",
    "def showLime(explanation, index, name):    \n",
    "    #temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=10, hide_rest=True)\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
    "    \n",
    "    plt.imshow(mark_boundaries(temp.astype('uint8'), mask))\n",
    "    cv2.imwrite(\"lime_16_\"+str(name)+\".jpg\", deprocess_image(mark_boundaries(temp / 2 + 0.5, mask)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Images//ILSVRC2012_val_00000008.JPEG\"\n",
    "\n",
    "preprocessed_input = load_image(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = VGG19(weights='imagenet')\n",
    "\n",
    "index = 1\n",
    "\n",
    "predictions = model.predict(preprocessed_input)\n",
    "top_1 = decode_predictions(predictions)[0][index]\n",
    "top_2 = decode_predictions(predictions)[0][index+1]\n",
    "top_3 = decode_predictions(predictions)[0][index+2]\n",
    "\n",
    "\n",
    "print('Predicted class:')\n",
    "print('%s (%s) with probability %.2f' % (top_1[1], top_1[0], top_1[2]))\n",
    "\n",
    "print('%s (%s) with probability %.2f' % (top_2[1], top_2[0], top_2[2]))\n",
    "print('%s (%s) with probability %.2f' % (top_3[1], top_3[0], top_3[2]))\n",
    "\n",
    "\n",
    "\n",
    "predicted_class = np.argsort(np.max(predictions, axis=0))[-(index+1)]\n",
    "print(predicted_class)\n",
    "\n",
    "\n",
    "cam, heatmap = grad_cam(model, preprocessed_input, predicted_class, \"block5_conv3\")\n",
    "cv2.imwrite(\"gradcam_19_\"+str(top_1[1])+\".jpg\", cam)\n",
    "\n",
    "register_gradient()\n",
    "guided_model = modify_backprop(model, 'GuidedBackProp', mods = 19)\n",
    "saliency_fn = compile_saliency_function(guided_model)\n",
    "saliency = saliency_fn([preprocessed_input, 0])\n",
    "gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
    "cv2.imwrite(\"guided_gradcam_19_\"+str(top_1[1])+\".jpg\", deprocess_image(gradcam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class:\n",
      "zebra (n02391049) with probability 0.88\n",
      "llama (n02437616) with probability 0.03\n",
      "sorrel (n02389026) with probability 0.03\n",
      "340\n",
      "zebra\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG16(weights='imagenet')\n",
    "index = 0\n",
    "\n",
    "predictions = model.predict(preprocessed_input)\n",
    "top_1 = decode_predictions(predictions)[0][index]\n",
    "top_2 = decode_predictions(predictions)[0][index+1]\n",
    "top_3 = decode_predictions(predictions)[0][index+2]\n",
    "\n",
    "\n",
    "print('Predicted class:')\n",
    "print('%s (%s) with probability %.2f' % (top_1[1], top_1[0], top_1[2]))\n",
    "\n",
    "print('%s (%s) with probability %.2f' % (top_2[1], top_2[0], top_2[2]))\n",
    "print('%s (%s) with probability %.2f' % (top_3[1], top_3[0], top_3[2]))\n",
    "\n",
    "predicted_class = np.argsort(np.max(predictions, axis=0))[-(index+1)]\n",
    "print(predicted_class)\n",
    "print(str(top_1[1]))\n",
    "\n",
    "cam, heatmap = grad_cam(model, preprocessed_input, predicted_class, \"block5_conv3\")\n",
    "cv2.imwrite(\"gradcam_16_mix_\"+str(top_1[1])+\".jpg\", cam)\n",
    "\n",
    "register_gradient()\n",
    "guided_model = modify_backprop(model, 'GuidedBackProp', mods = 16)\n",
    "saliency_fn = compile_saliency_function(guided_model)\n",
    "saliency = saliency_fn([preprocessed_input, 0])\n",
    "gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
    "#gradcam = heatmap[..., np.newaxis]\n",
    "\n",
    "cv2.imwrite(\"guided_gradcam_16_mix_\"+str(top_1[1])+\".jpg\", deprocess_image(gradcam))\n",
    "cv2.imwrite(\"normal_image_\"+str(top_1[1])+\".jpg\", deprocess_image(preprocessed_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('n02391049', 'zebra', 0.87616676)\n",
      "('n02437616', 'llama', 0.033408962)\n",
      "('n02389026', 'sorrel', 0.027120942)\n",
      "('n02423022', 'gazelle', 0.02127221)\n",
      "('n01518878', 'ostrich', 0.014710203)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c12e22f164348b9a8b10e268caa1eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#path = \"Images//parrot.jpeg\"\n",
    "exp = limePics(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showLime(exp, 0, str(top_1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explanation = exp\n",
    "\n",
    "#Select the same class explained on the figures above.\n",
    "ind =  explanation.top_labels[0]\n",
    "\n",
    "#Map each explanation weight to the corresponding superpixel\n",
    "dict_heatmap = dict(explanation.local_exp[ind])\n",
    "heatmap = np.vectorize(dict_heatmap.get)(explanation.segments) \n",
    "\n",
    "#Plot. The visualization makes more sense if a symmetrical colorbar is used.\n",
    "plt.imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(), vmax = heatmap.max())\n",
    "plt.colorbar()\n",
    "plt.savefig('heatmap_zebra_mix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INES",
   "language": "python",
   "name": "ines"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
